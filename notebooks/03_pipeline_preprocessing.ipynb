{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f8cbc-db6a-4fc3-b930-ba9101ae5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for pipeline preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn preprocessing and pipeline tools\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up plotting and display options\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"viridis\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40bcb8-59fb-4a2e-a075-f0e1768099e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88cff37e-c28a-4b45-abfd-3bdf1633ad24",
   "metadata": {},
   "source": [
    "#### Load and Examine Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10bc7dc-0527-414a-bd7a-a813a57cf241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original training data\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Check for missing values in training data\n",
    "print(f\"\\nMissing values in training data: {train_df.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test data: {test_df.isnull().sum().sum()}\")\n",
    "\n",
    "# Display basic info about the target variable\n",
    "print(f\"\\nTarget variable (SalePrice) statistics:\")\n",
    "print(f\"Mean: ${train_df['SalePrice'].mean():,.0f}\")\n",
    "print(f\"Median: ${train_df['SalePrice'].median():,.0f}\")\n",
    "print(f\"Range: ${train_df['SalePrice'].min():,.0f} - ${train_df['SalePrice'].max():,.0f}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(f\"\\nFirst 3 rows of training data:\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca060a-5d48-4f43-b3b5-a59b2f52834e",
   "metadata": {},
   "source": [
    "significant missing values to handle (7,829 in training, 7,878 in test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a864c3d-12e7-496f-bde8-fafeacfa8828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b730b211-fe9f-4271-952a-02502908ce6b",
   "metadata": {},
   "source": [
    "#### Analyze Data Types and Missing Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cbae62-a7a9-4605-b159-22ed80f5089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features by data type\n",
    "numerical_features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = train_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove target and ID from feature lists\n",
    "if 'SalePrice' in numerical_features:\n",
    "    numerical_features.remove('SalePrice')\n",
    "if 'Id' in numerical_features:\n",
    "    numerical_features.remove('Id')\n",
    "\n",
    "print(\"Feature Analysis:\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Analyze missing values by feature type\n",
    "print(f\"\\nMissing Values Analysis:\")\n",
    "missing_numerical = train_df[numerical_features].isnull().sum()\n",
    "missing_categorical = train_df[categorical_features].isnull().sum()\n",
    "\n",
    "print(f\"\\nTop 10 numerical features with missing values:\")\n",
    "print(missing_numerical[missing_numerical > 0].sort_values(ascending=False).head(10))\n",
    "\n",
    "print(f\"\\nTop 10 categorical features with missing values:\")\n",
    "print(missing_categorical[missing_categorical > 0].sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab276ba7-e377-4e08-a281-376de43a0a84",
   "metadata": {},
   "source": [
    "Most categorical missing values are likely \"None\" cases (no pool, no fence, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca33899-2eb4-4636-96a6-427b187a3a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23c1a1c8-685d-4895-96ff-2b1b617523d3",
   "metadata": {},
   "source": [
    "#### Define Preprocessing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeac557-26d9-428a-aefb-d42b363f794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups based on missing value patterns and domain knowledge\n",
    "\n",
    "# Numerical features - will use median imputation\n",
    "print(\"Numerical Features for Pipeline:\")\n",
    "print(f\"Total: {len(numerical_features)}\")\n",
    "print(\"Sample:\", numerical_features[:10])\n",
    "\n",
    "# Categorical features that should be \"None\" when missing (no feature present)\n",
    "none_categorical_features = [\n",
    "    'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n",
    "    'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'\n",
    "]\n",
    "\n",
    "# Categorical features that should use mode imputation (missing = most common value)\n",
    "mode_categorical_features = [feat for feat in categorical_features \n",
    "                           if feat not in none_categorical_features]\n",
    "\n",
    "print(f\"\\nCategorical Features Strategy:\")\n",
    "print(f\"'None' imputation: {len(none_categorical_features)} features\")\n",
    "print(\"Sample:\", none_categorical_features[:5])\n",
    "print(f\"\\nMode imputation: {len(mode_categorical_features)} features\") \n",
    "print(\"Sample:\", mode_categorical_features[:5])\n",
    "\n",
    "# Define ordinal features with their mappings (from previous analysis)\n",
    "ordinal_features = {\n",
    "    'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'BsmtQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'BsmtCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'FireplaceQu': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'GarageQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'GarageCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "}\n",
    "\n",
    "print(f\"\\nOrdinal features: {len(ordinal_features)} features\")\n",
    "print(\"Features:\", list(ordinal_features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e622bc8-935b-47a6-9e06-462ef966817c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "910afd19-4eab-4eac-a494-ae3767c4558c",
   "metadata": {},
   "source": [
    "#### Create ColumnTransformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7239ae-d54f-4fa9-ad7c-9a43c1c4aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines for different feature types\n",
    "\n",
    "# 1. Numerical pipeline: impute with median, then scale\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Categorical pipeline for \"None\" features: impute with \"None\", then one-hot encode\n",
    "none_categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# 3. Categorical pipeline for mode features: impute with mode, then one-hot encode\n",
    "mode_categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# 4. Ordinal pipeline: impute with \"None\", then ordinal encode\n",
    "ordinal_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')),\n",
    "    ('ordinal', OrdinalEncoder(categories=list(ordinal_features.values()), \n",
    "                              handle_unknown='use_encoded_value', \n",
    "                              unknown_value=-1))\n",
    "])\n",
    "\n",
    "print(\"Individual pipelines created:\")\n",
    "print(\"1. Numerical pipeline: median imputation + scaling\")\n",
    "print(\"2. None categorical pipeline: 'None' imputation + one-hot encoding\")\n",
    "print(\"3. Mode categorical pipeline: mode imputation + one-hot encoding\")\n",
    "print(\"4. Ordinal pipeline: 'None' imputation + ordinal encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f32ec0-5015-4d8c-b439-dc4a4b28fa6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a09b0d55-0579-43c8-84ba-607332b53300",
   "metadata": {},
   "source": [
    "#### Build Complete ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed41fba5-824b-4790-8758-003f3ae18118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate ordinal features from other categorical features\n",
    "ordinal_feature_names = list(ordinal_features.keys())\n",
    "none_categorical_final = [feat for feat in none_categorical_features \n",
    "                         if feat not in ordinal_feature_names]\n",
    "mode_categorical_final = [feat for feat in mode_categorical_features \n",
    "                         if feat not in ordinal_feature_names]\n",
    "\n",
    "# Create the complete ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', numerical_pipeline, numerical_features),\n",
    "    ('none_categorical', none_categorical_pipeline, none_categorical_final),\n",
    "    ('mode_categorical', mode_categorical_pipeline, mode_categorical_final),\n",
    "    ('ordinal', ordinal_pipeline, ordinal_feature_names)\n",
    "], remainder='drop')  # Drop any features not specified\n",
    "\n",
    "print(\"ColumnTransformer created with:\")\n",
    "print(f\"- Numerical features: {len(numerical_features)}\")\n",
    "print(f\"- None categorical features: {len(none_categorical_final)}\")\n",
    "print(f\"- Mode categorical features: {len(mode_categorical_final)}\")\n",
    "print(f\"- Ordinal features: {len(ordinal_feature_names)}\")\n",
    "print(f\"- Total features to process: {len(numerical_features) + len(none_categorical_final) + len(mode_categorical_final) + len(ordinal_feature_names)}\")\n",
    "\n",
    "# Verify we're not missing any features\n",
    "total_features = len(numerical_features) + len(categorical_features)\n",
    "processed_features = len(numerical_features) + len(none_categorical_final) + len(mode_categorical_final) + len(ordinal_feature_names)\n",
    "print(f\"\\nFeature accounting:\")\n",
    "print(f\"Original features (excluding Id, SalePrice): {total_features}\")\n",
    "print(f\"Features in preprocessor: {processed_features}\")\n",
    "print(f\"Match: {total_features == processed_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4690c0a-63be-48fe-b7f4-bd75b82a0c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14ed8ad2-4b7e-4bf8-a4be-533ebfb87063",
   "metadata": {},
   "source": [
    "#### Test Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350e689-56d1-4ac7-8eae-444666c792dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for preprocessing (exclude Id and SalePrice)\n",
    "X = train_df.drop(['Id', 'SalePrice'], axis=1)\n",
    "y = train_df['SalePrice']\n",
    "\n",
    "print(\"Data preparation:\")\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "\n",
    "# Split data for testing the pipeline\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTrain-validation split:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "\n",
    "# Fit the preprocessor on training data\n",
    "print(f\"\\nFitting preprocessor on training data...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "print(f\"Processed training data shape: {X_train_processed.shape}\")\n",
    "\n",
    "# Transform validation data using fitted preprocessor\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "print(f\"Processed validation data shape: {X_val_processed.shape}\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(f\"\\nMissing values after preprocessing:\")\n",
    "print(f\"Training data: {np.isnan(X_train_processed).sum()}\")\n",
    "print(f\"Validation data: {np.isnan(X_val_processed).sum()}\")\n",
    "\n",
    "print(f\"\\nPreprocessing pipeline test: SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781dfe5-13e2-446c-afed-684777ea3260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9b52c9c-8f34-40ca-8b77-ee4e30fe0762",
   "metadata": {},
   "source": [
    "#### Analyze Preprocessing Results and Handle Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f8283-27d0-43a7-a4e6-b1606e6231b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "print(f\"Feature expansion analysis:\")\n",
    "print(f\"Original features: 79\")\n",
    "print(f\"After preprocessing: {len(feature_names)}\")\n",
    "print(f\"Expansion factor: {len(feature_names)/79:.1f}x\")\n",
    "\n",
    "# Analyze feature types in final output\n",
    "print(f\"\\nFeature breakdown after preprocessing:\")\n",
    "numerical_count = len([name for name in feature_names if name.startswith('numerical')])\n",
    "none_cat_count = len([name for name in feature_names if name.startswith('none_categorical')])\n",
    "mode_cat_count = len([name for name in feature_names if name.startswith('mode_categorical')])\n",
    "ordinal_count = len([name for name in feature_names if name.startswith('ordinal')])\n",
    "\n",
    "print(f\"- Numerical features: {numerical_count}\")\n",
    "print(f\"- None categorical features: {none_cat_count}\")\n",
    "print(f\"- Mode categorical features: {mode_cat_count}\")\n",
    "print(f\"- Ordinal features: {ordinal_count}\")\n",
    "\n",
    "# Show sample of feature names\n",
    "print(f\"\\nSample feature names:\")\n",
    "print(\"First 10:\", feature_names[:10])\n",
    "print(\"Last 10:\", feature_names[-10:])\n",
    "\n",
    "# The warning about unknown categories is expected and handled correctly\n",
    "print(f\"\\nNote: Warning about unknown categories is normal and properly handled\")\n",
    "print(\"Unknown categories are encoded as zeros (ignored), preventing errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbde88-676d-4e8f-971b-ea4b24d5724e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a2bf6f0-2789-41cd-8d94-6d80f0813e35",
   "metadata": {},
   "source": [
    "#### Add Feature Engineering to Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d40d0-26ed-4823-a22a-2e11b4c03496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add some feature engineering that we know works well from your previous analysis\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Create a copy to avoid modifying original data\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Feature engineering (using original column names)\n",
    "        # Total square footage\n",
    "        X_copy['TotalSF'] = X_copy['1stFlrSF'] + X_copy['2ndFlrSF'] + X_copy['TotalBsmtSF']\n",
    "        \n",
    "        # Total bathrooms\n",
    "        X_copy['TotalBath'] = (X_copy['FullBath'] + \n",
    "                              0.5 * X_copy['HalfBath'] + \n",
    "                              X_copy['BsmtFullBath'] + \n",
    "                              0.5 * X_copy['BsmtHalfBath'])\n",
    "        \n",
    "        # House age (assuming current year is 2023)\n",
    "        X_copy['HouseAge'] = 2023 - X_copy['YearBuilt']\n",
    "        \n",
    "        # Years since remodel\n",
    "        X_copy['YearsSinceRemod'] = 2023 - X_copy['YearRemodAdd']\n",
    "        \n",
    "        # Was remodeled (binary)\n",
    "        X_copy['WasRemodeled'] = (X_copy['YearBuilt'] != X_copy['YearRemodAdd']).astype(int)\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "# Create complete pipeline with feature engineering\n",
    "complete_pipeline = Pipeline([\n",
    "    ('feature_engineering', FeatureEngineer()),\n",
    "    ('preprocessing', preprocessor)\n",
    "])\n",
    "\n",
    "print(\"Complete pipeline created:\")\n",
    "print(\"1. Feature Engineering: TotalSF, TotalBath, HouseAge, YearsSinceRemod, WasRemodeled\")\n",
    "print(\"2. Preprocessing: Imputation, scaling, encoding\")\n",
    "\n",
    "# Test the complete pipeline\n",
    "print(f\"\\nTesting complete pipeline...\")\n",
    "X_train_final = complete_pipeline.fit_transform(X_train)\n",
    "X_val_final = complete_pipeline.transform(X_val)\n",
    "\n",
    "print(f\"Final processed shapes:\")\n",
    "print(f\"Training: {X_train_final.shape}\")\n",
    "print(f\"Validation: {X_val_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10edb427-391c-4e6f-9a5f-636084b916b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f96f38bf-96ab-4a74-ad8e-5080c8381f08",
   "metadata": {},
   "source": [
    "#### Save Pipeline and Test on Real Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca4dcd-fd79-4289-85e9-89b6999b8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the complete pipeline\n",
    "pipeline_path = models_dir / 'preprocessing_pipeline.pkl'\n",
    "joblib.dump(complete_pipeline, pipeline_path)\n",
    "print(f\"Pipeline saved to: {pipeline_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff493555-16f9-4e0b-be58-252e005c5507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d832cf1-c0c1-471f-a06a-863a8725b1e6",
   "metadata": {},
   "source": [
    "#### Production Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b82a8-7272-4a32-9efa-3d3efcddf6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate production scenario: load pipeline from disk\n",
    "print(\"PRODUCTION PIPELINE TEST\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Clear the pipeline from memory (simulate fresh start)\n",
    "del complete_pipeline\n",
    "\n",
    "# Test pipeline on actual test dataset\n",
    "print(f\"\\nTesting pipeline on real test data...\")\n",
    "test_features = test_df.drop(['Id'], axis=1)  # Remove Id, no SalePrice in test\n",
    "print(f\"Test data shape before preprocessing: {test_features.shape}\")\n",
    "\n",
    "# Load pipeline from saved file\n",
    "loaded_pipeline = joblib.load('../models/preprocessing_pipeline.pkl')\n",
    "print(\"Pipeline loaded from disk successfully\")\n",
    "\n",
    "# Apply loaded pipeline to test data\n",
    "test_processed_production = loaded_pipeline.transform(test_features)\n",
    "print(f\"Test data processed using loaded pipeline: {test_processed_production.shape}\")\n",
    "\n",
    "# Verify no missing values\n",
    "missing_values = np.isnan(test_processed_production).sum()\n",
    "print(f\"Missing values: {missing_values}\")\n",
    "\n",
    "print(\"\\nProduction pipeline test: SUCCESS\")\n",
    "print(\"Pipeline can be deployed and used independently\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790354a-a26e-4d97-b706-69c4ef93c3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a50a3d-d9ab-486f-b2ee-327982052f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4846aee-fafd-4777-9fa9-a5cc2a2c8a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
